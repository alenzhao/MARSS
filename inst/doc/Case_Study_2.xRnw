\SweaveOpts{keep.source=TRUE, prefix.string=./figures/CS2-, eps=FALSE, split=TRUE}
\chapter{Case study 2: Combining multi-site and subpopulation data to estimate regional population dynamics}
\label{chap:CStrend}
\chaptermark{Combining multi-site and subpopulation data}

<<RUNFIRST, echo=F, include.source=F>>=
require(MARSS)
options(prompt=" ", continue=" ")
@
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The problem}

In this case study, we will use multivariate state-space models\index{MARSS model!multivariate example} to combine surveys from multiple regions (or sites) into one estimate of the average long-term population growth rate and the year-to-year variability in that growth rate.  Note this is not quite the same as estimating the `trend'; `trend' often means what population change happened, whereas the long-term population growth rate refers to the underlying population dynamics.  We will use as our example a dataset from harbor seals in Puget Sound, Washington, USA.  

We have five regions (or sites) where harbor seals were censused from 1978-1999 while hauled out of land\footnote{Jeffries et al. 2003.  Trends and status of harbor seals in Washington State: 1978-1999. Journal of Wildlife Management 67(1):208--219 }.  During the period of this dataset, harbor seals were recovering steadily after having been reduced to low levels by hunting prior to protection.  The methodologies were consistent throughout the 20 years of the data but we do not know what fraction of the population that each region represents nor do we know the observation-error variance for each region.  Given differences between behaviors of animals in different regions and the numbers of haul-outs in each region, the observation errors may be quite different.  The regions have had different levels of sampling; the best sampled region has only 4 years missing while the worst has over half the years missing.  

Figure~\ref{fig:CS2.fig1} shows the data.  The numbers on each line denote the different regions:
<<noshowlegend, echo=FALSE>>=
d=harborSealWA
legendnames = (unlist(dimnames(d)[2]))[2:ncol(d)]
for(i in 1:length(legendnames)) cat(paste(i,legendnames[i],"\n",sep=" "))
@
%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
\begin{figure}[htp]
\begin{center}
<<label=fig1,fig=TRUE,echo=FALSE,width=5,height=5>>=
d=harborSealWA
dat = d[,2:ncol(d)] #first col is years
dat[dat==-99]=NA #replace the -99s with NA for plotting
x = d[,1] #first col is years
n = ncol(dat) #num time series

#set up the graphical parameters to give each data a unique line, color and width
options(warn=-99)
ltys=matrix(1,nrow=n)
cols=matrix(1:4,nrow=n)
lwds=matrix(1:2,nrow=n)
pchs=matrix(as.character(c(1:n)),nrow=n)
options(warn=0)

matplot(x,dat,xlab="",ylab="log(counts)",type="b",pch=pchs,lty=ltys,col=cols,lwd=lwds,bty="L")
title("Puget Sound Harbor Seal Surveys")
@
\end{center}
\caption{Plot of the of the count data from the five harbor seal regions (Jeffries et al. 2003).  Each region is an index of the total harbor seal population, but the bias (the difference between the index and the true population size) for each region is unknown. }
\label{fig:CS2.fig1}
\end{figure}
%~~~~~~~~~~~~~~~~~~~~~~~~~

For this case study, we will assume that the underlying population process is a stochastic exponential growth process with rates of increase that were not changing through 1978-1999.  However, we are not sure if all five regions sample a single ``total Puget Sound'' population or if there are  independent subpopulations.  You are going to estimate the long-term population growth rate using different assumptions about the population structures (one big population versus multiple smaller ones) and observation error structures to see how your assumptions change your estimates.  

The data for this case study are in the MARSS package.   The data have time running down the rows and years in the first column. We need time across the columns for the \verb@MARSS()@ function, so we will transpose the data:
<<Cs2-readindata, echo=TRUE, keep.source=TRUE>>=
dat=t(harborSealWA) #Transpose
years = dat[1,] #[1,] means row 1
n = nrow(dat)-1
dat = dat[2:nrow(dat),] #no years
@
If you needed to read data in from a comma-delimited or tab-delimited file, these are the commands to do that:
<<Cs2-readtable, eval=FALSE>>=
dat = read.csv("datafile.csv",header=TRUE)
dat = read.table("datafile.csv",header=TRUE)
@

\noindent The years (\verb@years@) are in row 1 of \texttt{dat} and the logged data are in the rest of the rows. The number of observation time series (\verb@n@) is the number of rows in \verb@dat@ minus 1 (for years row).  Let's look at the first few years of data:
<<Cs2-showdata>>=
print(harborSealWA[1:8,], digits=3)
@
The \texttt{-99}'s in the data are missing values.  The algorithm will ignore those values.  

\section{Analysis assuming a single total Puget Sound population}
The first step in a state-space modeling analysis is to specify the population structure and how the regions relate to that structure.  The general state-space model is
\begin{eqnarray}
\xx_t = \BB \xx_{t-1} + \uu + \vv_t, \textrm{ where } \vv_t \sim \MVN(0,\QQ ) \label{eq:MSSMX} \\
\yy_t = \ZZ \xx_t + \aa + \ww_t, \textrm{ where } \ww_t \sim \MVN(0,\RR ) \label{eq:MSSMY}
\end{eqnarray}
where all the bolded symbols are matrices.  To specify the structure of the population and observations, we will specify what those matrices look like.

\subsection{A single population process, $\xx$}
When we are looking at data over a large geographic region, we might make the assumption that the different census regions are measuring a single population if we think animals are moving sufficiently such that the whole area (multiple regions together) is "well-mixed".  We write a model of the total  population abundance as:
%~~~~~~~~~~~~~~~~~~~~~~~~~
\begin{equation}
n_t = \exp(u + v_t) n_{t-1},
\label{eq:expstoc}\end{equation}
%~~~~~~~~~~~~~~~~~~~~~~~~~
where $n_t$ is the total count in year $t$, $u$ is the mean population growth rate, and $v_t$ is the deviation from that average in year $t$. 
We then take the log of both sides and write the model in log space:
%~~~~~~~~~~~~~~~~~~~~~~~~~
\begin{equation}
x_t = x_{t-1} + u + v_t, \textrm{ where } v_t \sim \N(0,q)
\label{eq:seg}
\end{equation}
%~~~~~~~~~~~~~~~~~~~~~~~~~
$x_t=\log{n_t}$. When there is one effective population, there is one $x$, therefore $\xx_t$ is a $1 \times 1$ matrix.  There is one population growth rate ($u$) and there is one process variance ($q$).  Thus $\uu$ and $\QQ$ are $1 \times 1$ matrices.   

\subsection{The observation process, $\yy$}
For this first analysis, we assume that all five regional time series are observing this one population trajectory but they are scaled up or down relative to that trajectory.   In effect, we think that animals are moving around a lot and our regional samples are some fraction of the population.  There is year-to-year variation in the fraction in each region, just by chance.  Notice that under this analysis, we do not think the regions represent independent subpopulations but rather independent observations of one population.
Our model for the data, $\yy_t = \ZZ \xx_t + \aa + \ww_t$, is written as:
%~~~~~~~~~~~~~~~~~~~~~~~~~
\begin{equation}
 \left[ \begin{array}{c}
    y_{1,t} \\
    y_{2,t} \\
    y_{3,t} \\
    y_{4,t} \\
    y_{5,t} \end{array} \right] = 
    \left[ \begin{array}{c}
    1\\
    1\\
    1\\
    1\\
    1\end{array} \right] x_t +  
    \left[ \begin{array}{c}
    0 \\
    a_2 \\
    a_3 \\
    a_4 \\
    a_5 \end{array} \right] + 
    \left[ \begin{array}{c}
    w_{1,t} \\
    w_{2,t} \\
    w_{3,t} \\
    w_{4,t} \\
    w_{5,t} \end{array} \right] 
 \label{eq:meas}\end{equation}
%~~~~~~~~~~~~~~~~~~~~~~~~~

Each $y_{i}$ is the time series for a different region.  The $a$'s are the bias between the regional sample and the total population.  The $a$'s are scaling (or intercept-like) parameters\footnote{To get rid of the $a$'s, we scale multiple observation time series against each other; thus one $a$ will be fixed at 0. Estimating the bias between regional indices and the total population is important for getting an estimate of the total population size.  The type of time-series analysis that we are doing here (trend analysis) is not useful for estimating $a$'s.  Instead to get $a$'s one would need some type of mark-recapture data.  However, for trend estimation, the $a$'s are not important.  The regional observation variance captures increased variance due to a regional estimate being a smaller sample of th total population.}.  We allow that each region could have a unique observation variance and that the observation errors are independent between regions.  Lastly, we assume that the observations errors on log(counts) are normal and thus the errors on (counts) are log-normal.\footnote{The assumption of normality is not unreasonable since these regional counts are the sum of counts across multiple haul-outs.}  

We specify independent observation errors with unique variances by specifying that the $w$'s come from a multivariate normal distribution with variance-covariance matrix $\RR$ ($\ww \sim \MVN(0,\RR)$), where
\begin{equation}
\RR = \left[ \begin{array}{ccccc}
    r_1 & 0 & 0 & 0 & 0 \\
    0 & r_2 & 0 & 0 & 0 \\
    0 & 0 & r_3 & 0 & 0 \\
    0 & 0 & 0 & r_4 & 0 \\
    0 & 0 & 0 & 0 & r_5 \end{array} \right]
\end{equation}

$\ZZ$ specifies which observation time series, $y_{i,1:T}$, is associated with which population trajectory, $x_{j,1:T}$.  $\ZZ$ is like a look up table with 1 row for each of the $n$ observation time series and 1 column for each of the $m$ population trajectories.  A 1 in row $i$ column $j$ means that observation time series $i$ is measuring state process $j$.  Otherwise the value in $\ZZ_{ij}=0$.  Since we have only 1 population trajectory, all the regions must be measuring that one population trajectory.  Thus $\ZZ$ is $n \times 1$:
\begin{equation}
    \ZZ = \left[ \begin{array}{c}
    1\\
    1\\
    1\\
    1\\
    1\end{array} \right]
\end{equation}

\subsection{Set the constraints for \texttt{MARSS}}
Now that we have specified our state-space model, we set the arguments that will tell the function \verb@MARSS()@ the structure of our model.  We do this by passing in the argument \verb@constraint@ to \verb@MARSS()@.  \verb@constraint@ is a list which specifies any constraints on $\ZZ$, $\uu$, $\QQ$, etc.  The function call will now look like:
<<Cs2-funccallwithconstraints, eval=FALSE>>=
kem = MARSS(dat, constraint=list(Z=Z.constraint, U=U.constraint,
      Q=Q.constraint, R=R.constraint) ) 
@

First we set the $\ZZ$ constraint.  We need to tell the \texttt{MARSS} function that $\ZZ$ is a $5 \times 1$ matrix of 1s (as in Equation \ref{eq:meas}).  We can do this two ways. We can pass in \verb@Z.constraint@ as a matrix of ones, \verb@matrix(1,5,1)@, just like in  Equation \eqref{eq:meas} or we can pass in a vector of five factors,  \verb@factor(c(1,1,1,1,1))@. The $i$-th factor specifies which population trajectory the $i$-th observation time series belongs to.  Since there is only one population trajectory in this first analysis, we will have a vector of five $1$'s: every observation time series is measuring the first, and only, population trajectory. 
<<Cs2.setwhichPop, eval=TRUE>>=
Z.constraint = factor(c(1,1,1,1,1))
@
Note, the vector (the \verb@c()@ bit) must be wrapped in \verb@factor()@ so that \verb@MARSS@ recognizes what it is.  You can use either numeric or character vectors: \verb@c(1,1,1,1,1)@ is the same as \verb@c("PS","PS","PS","PS","PS")@.

Next we specify that the $\RR$ variance-covariance matrix only has terms on the diagonal (the variances) with the off-diagonal terms (the covariances) equal to zero\footnote{For the EM function in the MARSS 1.0 package, the measurement errors must be uncorrelated if there are missing values in the data.}:  
<<Cs2-setvarcov1>>=
R.constraint = "diagonal and unequal" 
@
The `and unequal' part specifies that the variances are allowed to be unique on the diagonal.  If we wanted to force the observation variances to be equal at all regions, we would use \verb@"diagonal and equal"@.  

For the first analysis, we only need to set constraints on $\ZZ$ and $\RR$.  Since there is only one population, there is only one $\uu$ and $\QQ$ (they are scalars), so there are no constraints to set on them.

%~~~~~~~~~~~~~~~~~~~~~~~~~
\begin{figure}[htp]
\begin{center}
<<Cs2-fig2,fig=TRUE,echo=FALSE,width=6,height=6, results=hide>>=
kem1 = MARSS(dat, constraint=
       list(Z=factor(c(1,1,1,1,1)), R="diagonal and unequal"))
matplot(years, t(dat),xlab="",ylab="index of log abundance",pch=c("1","2","3","4","5"),ylim=c(5,9),bty="L")
lines(years,kem1$states-1.96*kem1$states.se,type="l",lwd=1,lty=2,col="red")
lines(years,kem1$states+1.96*kem1$states.se,type="l",lwd=1,lty=2,col="red")
lines(years,kem1$states,type="l",lwd=2)
title("Observations and total population estimate",cex.main=.9)
@
\end{center}
\caption{Plot of the estimate of ``log total harbor seals in Puget Sound'' (minus the unknown bias for time series 1) against the data. The estimate of the total seal count has been scaled relative to the first time series. The 95\% confidence intervals on the population estimates are the dashed lines.  These are not the confidence intervals on the observations, and the observations (the numbers) will not fall between the confidence interval lines.}
\label{fig:CS2.fig2}
\end{figure}
%~~~~~~~~~~~~~~~~~~~~~~~~~

\subsection{The \texttt{MARSS()} output} 

The output from \verb@MARSS()@, here assigned the name \verb@kem@, is a list of objects. To see all the objects in it, type:
<<noshow-listkem1, eval=FALSE>>=
names(kem1)
@
The maximum-likelihood estimates of 
``total harbor seal population'' scaled to the first observation data series (Figure \ref{fig:CS2.fig2}) are in \verb@kem1$states@, and \verb@kem1$states.se@ are the standard errors on those estimates.  To get 95\% confidence intervals, use \verb@kem1$states +/- 1.96*kem1$states.se@.
Figure \ref{fig:CS2.fig2} shows a plot of \verb@kem1$states@ with its 95\% confidence intervals over the data.  Because \verb@kem1$states@ has been scaled relative to the first time series, it is on top of that time series.     One of the biases, the $a$s, cannot be estimated and arbitrarily our algorithm choses $a_1=0$, so the population estimate is scaled to the first observation time series.   

The estimated parameters are a list: \verb@kem1$par@.  To get the element \texttt{U} of that list, which is the estimated long-term population growth rate, type in \verb@kem1$par$U@. Multiply by 100 to get the percent increase per year.  The estimated process variance is given by \verb@kem1$par$Q@.

The log-likelihood of the fitted model is in \verb@kem1$logLik@.  We estimated  one initial $x$ ($t=1$), one process variance, one $u$, four $a$'s, and five observation variances's. So $K=12$ parameters.  The AIC\index{model selection!AIC} of this model is $-2 \times loglike + 2K$, which we can show by typing \verb@kem1$AIC@.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}{Fit the single population model}
\label{CS2.ex1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Analyze the harbor seal data using the single population model (Equations \ref{eq:seg} and \ref{eq:meas}).  The code for Example \ref{CS2.ex1} shows you how to input data and send it to the function \verb@MARSS()@.  As you run the examples, add the estimates to the table at the end of the chapter so you can compare estimates across the examples.

\exbegin{Example \ref{CS2.ex1} code}
\newline
{\footnotesize{Type \texttt{show.doc(MARSS, Case\_study\_2.R)} to open a file with all the example code.}}
<<label=Cs2_Exercise1,fig=FALSE,echo=TRUE,keep.source=TRUE, results=hide>>=
#Read in data
dat=t(harborSealWA) #Transpose since MARSS needs time ACROSS columns
years = dat[1,]      
n = nrow(dat)-1
dat = dat[2:nrow(dat),]
legendnames = (unlist(dimnames(dat)[1]))

#estimate parameters
Z.constraint = factor(c(1,1,1,1,1))
R.constraint = "diagonal and unequal" 
kem1 = MARSS(dat, constraint=
  list(Z=Z.constraint, R=R.constraint))

#make figure
matplot(years, t(dat),xlab="",ylab="index of log abundance",
  pch=c("1","2","3","4","5"),ylim=c(5,9),bty="L")
lines(years,kem1$states-1.96*kem1$states.se,type="l",
  lwd=1,lty=2,col="red")
lines(years,kem1$states+1.96*kem1$states.se,type="l",
  lwd=1,lty=2,col="red")
lines(years,kem1$states,type="l",lwd=2)
title("Observations and total population estimate",cex.main=.9)

#show params
kem1$par
kem1$logLik
kem1$AIC
@
\exend
\end{example}

\section{Changing the assumption about the observation variances}
The variable \verb@kem1$par$R@ contains the estimates of the observation error variances.  It is a matrix.  Here is $\RR$ from Example \ref{CS2.ex1}: 
<<Cs2-analysis2obsvariance>>=
print(kem1$par$R, digits=3)
@
Notice that the variances along the diagonal are all different---we estimated five unique observation variances.  We might be able to improve the fit (relative to the number of estimated parameters) by assuming that the observation variance is equal across regions but the errors are independent.  This means we estimate one observation variance instead of five.  This is a fairly standard assumption for data that come from the uniform survey methodology\footnote{By the way, this is not a good assumption for these data since the number haul-outs in each region varies and the regional counts are the sums across all haul-outs in a region.  We will see that this is a poor assumption when we look at the AIC values.}.

To impose this constraint, we set the $\RR$ constraint to 
<<label=Cs2-setgroups>>=
R.constraint="diagonal and equal"
@
This tells \verb@MARSS@ that all the $r$'s along the diagonal in $\RR$ are the same.  To fit this model to the data, call \texttt{MARSS()} as:
<<Cs2-fitanalysis2, results=hide>>=
Z.constraint = factor(c(1,1,1,1,1))
R.constraint = "diagonal and equal" 
kem2 = MARSS(dat, constraint=
       list(Z=Z.constraint, R=R.constraint))
@

We estimated  one initial $x$, one process variance, one $u$, four $a$'s, and one observation variance. So $K=8$ parameters.
The AIC for this new model compared to the old model with five observation variances is:
<<Cs2-AIC2, eval=TRUE>>=
c(kem1$AIC,kem2$AIC)
@
A smaller AIC means a better model\index{model selection!AIC}.  The difference between the one observation variance versus the unique observation variances is >10, suggesting that the unique observation variances model is better.  Go ahead and type in the \R code.  Then add the parameter estimates to the table at the back.

\index{diagnostics}One of the key diagnostics when you are comparing fits from multiple models, it to examine whether the model is flexible enough to fit the data.  You do this by looking for temporal trends in the the residuals between the estimated population states (e.g. \verb@kem2$states@) and the data.  In Figure \ref{fig:CS2.resids2}, the residuals for the second analysis are shown.  Ideally, these residuals should not have a temporal trend. They should look cloud-like.  The fact that the residuals have a strong temporal trend is an indication that our one population model is too restrictive for the data\footnote{When comparing models via AIC, it is important that you only compare models that are flexible enough to fit the data.  Fortunately if you neglect to do this, the inadequate models will usually have very high AICs and fall out of the mix anyhow.}.  

%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
\begin{figure}[htp]
\begin{center}
<<Cs2-fig5,fig=TRUE,echo=FALSE,width=5,height=5>>=
plotdat = t(dat); plotdat[plotdat == -99] = NA;
matrix.of.biases = matrix(kem2$par$A,nrow=nrow(plotdat),ncol=ncol(plotdat),byrow=T)
xs = matrix(kem2$states,nrow=dim(plotdat)[1],ncol=dim(plotdat)[2],byrow=F)
resids = plotdat-matrix.of.biases-xs
par(mfrow=c(2,3))
for(i in 1:n){
	plot(resids[!is.na(resids[,i]),i],ylab="residuals")
	title(legendnames[i])
	}
par(mfrow=c(1,1))
@
\end{center}
\caption{Residuals for the model with a single population. The plots of the residuals should not have trends with time, but they do...  This is an indication that the single population model is inconsistent with the data.  The code to make this plot is given in the script file for this case study.}
\label{fig:CS2.resids2}
\end{figure}
%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}{Fit a model with shared observation variances}
\label{CS2.ex2}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Analyze the data using the same population model as in Example \ref{CS2.ex1}, but constrain the $\RR$ matrix so that all five census regions have the same observation variance.  The Example \ref{CS2.ex2} code shows you how to do this.  It also shows you how to make the diagnostics figure (Figure \ref{fig:CS2.resids2}).

\exbegin{Example \ref{CS2.ex2} code}
\newline
{\footnotesize{Type \texttt{show.doc(MARSS, Case\_study\_2.R)} to open a file with all the example code.}}
<<Cs2_Exercise2,fig=FALSE,echo=TRUE,keep.source=TRUE, results=hide>>=
#fit model
Z.constraint = factor(c(1,1,1,1,1))
R.constraint = "diagonal and equal" 
kem2 = MARSS(dat, constraint=
  list(Z=Z.constraint, R=R.constraint))

#show parameters
kem2$par$U       #population growth rate
kem2$par$Q       #process variance
kem2$par$R[1,1]  #observation variance
kem2$logLik #log likelihood
c(kem1$AIC,kem2$AIC)

#plot residuals
plotdat = t(dat); plotdat[plotdat == -99] = NA;
matrix.of.biases = matrix(kem2$par$A,
  nrow=nrow(plotdat),ncol=ncol(plotdat),byrow=T)
xs = matrix(kem2$states,
  nrow=dim(plotdat)[1],ncol=dim(plotdat)[2],byrow=F)
resids = plotdat-matrix.of.biases-xs
par(mfrow=c(2,3))
for(i in 1:n){
  plot(resids[!is.na(resids[,i]),i],ylab="residuals")
  title(legendnames[i])
}
par(mfrow=c(1,1))
@
\exend
\end{example}

\section{Analysis assuming north and south subpopulations}
For the third analysis, we will change our assumption about the structure of the population.  We will assume that there are two subpopulations, north and south, and that regions 1 and 2 (Strait of Juan de Fuca and San Juan Islands) fall in the north subpopulation and regions 3, 4 and 5 fall in the south subpopulation.  For this analysis, we will assume that these two subpopulations share their growth parameter, $u$, and process variance, $q$, since they share a similar environment and prey base.  However we postulate that because of fidelity to natal rookeries for breeding, animals do not move much year-to-year between the north and south and the two subpopulations are independent.  

We need to write down the state-space model to reflect this population structure.  There are two subpopulations, $x_n$ and $x_s$, and they have the same growth rate $u$:
%~~~~~~~~~~~~~~~~~~~~~~~~~
\begin{equation}
 \left[ \begin{array}{c}
    x_{n,t} \\
    x_{s,t} \end{array} \right] = 
 \left[ \begin{array}{c}
    x_{n,t-1} \\
    x_{s,t-1} \end{array} \right]+
    \left[ \begin{array}{c}
    u \\
    u \end{array} \right] +
    \left[ \begin{array}{c}
    v_{n,t} \\
    v_{s,t} \end{array} \right]
\label{eq:procanal3}
\end{equation}
%~~~~~~~~~~~~~~~~~~~~~~~~~
We specify that they are independent by specifying that their year-to-year population fluctuations (their process errors) come from a multivariate normal with no covariance:
\begin{equation}
 \left[ \begin{array}{c}
    v_{n,t} \\
    v_{s,t} \end{array} \right] \sim 
 MVN\left(\left[ \begin{array}{c}
    0 \\
    0 \end{array} \right], 
    \left[ \begin{array}{cc}
    q & 0 \\
    0 & q \end{array} \right] \right)
\end{equation}

For the observation process, we use the $\ZZ$ matrix to associate the regions with their respective $x_n$ and $x_s$ values:
\begin{equation}
    \left[ \begin{array}{c}
    y_{1,t} \\
    y_{2,t} \\
    y_{3,t} \\
    y_{4,t} \\
    y_{5,t} \end{array} \right] = 
    \left[ \begin{array}{cc}
    1 & 0 \\
    1 & 0 \\
    0 & 1 \\
    0 & 1 \\
    0 & 1 \end{array} \right] \left[ \begin{array}{c}
    x_{n,t} \\
    x_{s,t} \end{array} \right] +
    \left[ \begin{array}{c}
    0 \\
    a_2 \\
    0 \\
    a_4 \\
    a_5 \end{array} \right] +
    \left[ \begin{array}{c}
    w_{1,t} \\
    w_{2,t} \\
    w_{3,t} \\
    w_{4,t} \\
    w_{5,t} \end{array} \right]
 \label{eq:obsanal3}
 \end{equation}
%~~~~~~~~~~~~~~~~~~~~~~~~~

\subsection{Specifying the \texttt{MARSS()} arguments}
We need to change the $\ZZ$ constraint to specify that there are two subpopulations (north and south), and that regions 1 and 2 are in the north subpopulation and regions 3,4 and 5 are in the south subpopulation.  There are a few ways, we can specify this $\ZZ$ matrix for \verb@MARSS()@:
<<label=Cs2-analysis3-Z, eval=FALSE>>=
Z.constraint = matrix(c(1,1,0,0,0,0,0,1,1,1),5,2) 
Z.constraint = factor(c(1,1,2,2,2)) 
Z.constraint = factor(c("N","N","S","S","S")) 
@
Which you choose is a matter of preference as the all specify the same form for $\ZZ$.

We also want to specify that the $u$'s are the same for each subpopulation and that $\QQ$ is diagonal with equal $q$'s.  To do this, we set
<<label=Cs2-analysis3-UQ, eval=FALSE>>=
U.constraint = "equal" 
Q.constraint = "diagonal and equal" 
@

\noindent This says that there is one $u$ and one $q$ parameter and both subpopulations share it (if we wanted the $u$'s to be different, we would use \verb@U.constraint="unequal"@ or leave off the $\uu$ constraint since the default behavior is \verb@U.constraint="unequal"@).  

Now we specify the new constraints and fit this model to the data:  
<<label=Cs2-fitanalysis3,keep.source=TRUE>>=
Z.constraint = factor(c(1,1,2,2,2))
U.constraint = "equal" 
Q.constraint = "diagonal and equal"
R.constraint = "diagonal and equal" 
kem3 = MARSS(dat, constraint=list(Z=Z.constraint, 
   R=R.constraint, U=U.constraint, Q=Q.constraint))
@

Figure \ref{fig:CS2.resids3} shows the residuals for the two subpopulations case.  The residuals look better (more cloud-like) but the Hood Canal residuals are still temporally correlated.

%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
\begin{figure}[htp]
\begin{center}
<<label=Cs2-residuals-analysis3-fig6,fig=TRUE,echo=FALSE,width=5,height=5>>=
plotdat = t(dat); plotdat[plotdat == -99] = NA;
matrix.of.biases = matrix(kem3$par$A,nrow=nrow(plotdat),ncol=ncol(plotdat),byrow=T)
par(mfrow=c(2,3))
for(i in 1:n){
	j=c(1,1,2,2,2)
	xs = kem3$states[j[i],]
	resids = plotdat[,i]-matrix.of.biases[,i]-xs
	plot(resids[!is.na(resids)],ylab="residuals")
	title(legendnames[i])
	}
par(mfrow=c(1,1))
@
\end{center}
\caption{The residuals for the analysis with a north and south subpopulation.  The plots of the residuals should not have trends with time.  Compare with the residuals for the analysis with one subpopulation.}
\label{fig:CS2.resids3}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}{Fit a model with north and south subpopulations}
\label{CS2.ex3}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Analyze the data using a model with two subpopulations, northern and southern.  Assume that the subpopulation are independent (diagonal $\QQ$), however let each subpopulation share the same population parameters, $u$ and $q$.  The Example \ref{CS2.ex3} code shows how to set the \texttt{MARSS()} arguments for this case.

\exbegin{Example \ref{CS2.ex3} code}
\newline
{\footnotesize{Type \texttt{show.doc(MARSS, Case\_study\_2.R)} to open a file with all the example code.}}
<<label=Cs2_Exercise3,fig=FALSE,echo=TRUE,keep.source=TRUE, results=hide>>=
#fit model
Z.constraint = factor(c(1,1,2,2,2))
U.constraint = "equal" 
Q.constraint = "diagonal and equal"
R.constraint = "diagonal and equal" 
kem3 = MARSS(dat, constraint=list(Z=Z.constraint, 
  R=R.constraint, U=U.constraint, Q=Q.constraint))
#plot residuals
plotdat = t(dat); plotdat[plotdat == -99] = NA;
matrix.of.biases = matrix(kem3$par$A,
  nrow=nrow(plotdat),ncol=ncol(plotdat),byrow=T)
par(mfrow=c(2,3))
for(i in 1:n){
  j=c(1,1,2,2,2)
  xs = kem3$states[j[i],]
  resids = plotdat[,i]-matrix.of.biases[,i]-xs
  plot(resids[!is.na(resids)],ylab="residuals")
  title(legendnames[i])
}
par(mfrow=c(1,1))
@
\exend
\end{example}

\section{Using \texttt{MARSS()} to fit other population structures}
Now work through a number of different structures and fill out the table at the back of this case study.  At the end you will see how your estimation of the mean population growth rate varies under different assumptions about the population and the data. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}{Five subpopulations}
\label{CS2.ex4}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Analyze the data using a model with five subpopulations, where each of the five census regions is sampling one of the subpopulations.  Assume that the subpopulation are independent (diagonal $\QQ$), however let each subpopulation share the same population parameters, $u$ and $q$.  The Example \ref{CS2.ex4} code shows how to set the \texttt{MARSS()} arguments for this case.  You can use \verb@R.constraint="diagonal and equal"@ to make all the observation variances equal.

\exbegin{Example \ref{CS2.ex4} code}
\newline
{\footnotesize{Type \texttt{show.doc(MARSS, Case\_study\_2.R)} to open a file with all the example code.}}
<<label=Cs2_Exercise4,fig=F,echo=T,keep.source=T, results=hide>>=
Z.constraint=factor(c(1,2,3,4,5))
U.constraint="equal"
Q.constraint="diagonal and equal"
R.constraint="diagonal and unequal"
kem=MARSS(dat, constraint=list(Z=Z.constraint, 
  U=U.constraint, Q=Q.constraint, R=R.constraint) )
@
\exend
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}{Two subpopulations with different population parameters}
\label{CS2.ex5}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Analyze the data using a model that assumes that the Strait of Juan de Fuca and San Juan Islands census regions represent a northern Puget Sound subpopulation, while the other three regions represent a southern Puget Sound subpopulation. This time assume that each population trajectory (north and south) has different $u$ and $q$ parameters: $u_n$,$u_s$ and $q_n$,$q_s$. Also assume that each of the five census regions has a different observation variance.  Try to write your own code.  If you get stuck (or want to check your work, you can open a script file with all the Case Study 2 examples by typing \texttt{show.doc(MARSS, Case\_study\_2.R)} at the R command line.

In math form, this model is:
\begin{equation}
 \left[ \begin{array}{c}
    x_{n,t} \\
    x_{s,t} \end{array} \right] = 
 \left[ \begin{array}{c}
    x_{n,t-1} \\
    x_{s,t-1} \end{array} \right]+
    \left[ \begin{array}{c}
    u_n \\
    u_s \end{array} \right] +
    \left[ \begin{array}{c}
    v_{n,t} \\
    v_{s,t} \end{array} \right],
    \left[ \begin{array}{c}
    v_{n,t} \\
    v_{s,t} \end{array} \right] \sim \MVN\left(0,
    \left[ \begin{array}{cc}
    q_n & 0\\
    0 & q_s \end{array} \right]\right)
\end{equation}
\begin{equation}
\left[ \begin{array}{c}
    y_{1,t} \\
    y_{2,t} \\
    y_{3,t} \\
    y_{4,t} \\
    y_{5,t} \end{array} \right] = 
    \left[ \begin{array}{cc}
    1 & 0 \\
    1 & 0 \\
    0 & 1 \\
    0 & 1 \\
    0 & 1 \end{array} \right] \left[ \begin{array}{c}
    x_{n,t} \\
    x_{s,t} \end{array} \right] +
    \left[ \begin{array}{c}
    0 \\
    a_2 \\
    0 \\
    a_4 \\
    a_5 \end{array} \right] +
    \left[ \begin{array}{c}
    w_{1,t} \\
    w_{2,t} \\
    w_{3,t} \\
    w_{4,t} \\
    w_{5,t} \end{array} \right]
 \label{eq:obsanal3}
 \end{equation}
 \end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}{Hood Canal treated separately but covaries with others}
\label{CS2.ex6}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Analyze the data using a model with two subpopulations with the  divisions being Hood Canal versus everywhere else.  In math form, this model is:
\begin{equation}
 \left[ \begin{array}{c}
    x_{p,t} \\
    x_{h,t} \end{array} \right] = 
 \left[ \begin{array}{c}
    x_{p,t-1} \\
    x_{h,t-1} \end{array} \right]+
    \left[ \begin{array}{c}
    u_p \\
    u_h \end{array} \right] +
    \left[ \begin{array}{c}
    v_{p,t} \\
    v_{h,t} \end{array} \right],
    \left[ \begin{array}{c}
    v_{p,t} \\
    v_{h,t} \end{array} \right] \sim \MVN\left(0,
    \left[ \begin{array}{cc}
    q & c\\
    c & q \end{array} \right]\right)
\end{equation}
\begin{equation}
\left[ \begin{array}{c}
    y_{1,t} \\
    y_{2,t} \\
    y_{3,t} \\
    y_{4,t} \\
    y_{5,t} \end{array} \right] = 
    \left[ \begin{array}{cc}
    1 & 0 \\
    1 & 0 \\
    1 & 0 \\
    1 & 0 \\
    0 & 1 \end{array} \right] \left[ \begin{array}{c}
    x_{p,t} \\
    x_{h,t} \end{array} \right] +
    \left[ \begin{array}{c}
    0 \\
    a_2 \\
    a_3 \\
    a_4 \\
    0 \end{array} \right] +
    \left[ \begin{array}{c}
    w_{1,t} \\
    w_{2,t} \\
    w_{3,t} \\
    w_{4,t} \\
    w_{5,t} \end{array} \right]
 \label{eq:obsanal3}
 \end{equation}
 
To specify that $\QQ$ has one value on the diagonal (one variance) and one value on the off-diagonal (covariance) you can specify \verb@Q.constraint@ two ways:
<<norun3, eval=FALSE>>=
Q.constraint = "equalvarcov" 
Q.constraint = matrix(c("q","c","c","q"),2,2)
@  
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{example}{Three subpopulations with shared parameter values}
\label{CS2.ex7}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Analyze the data using a model with three subpopulations as follows: north (regions 1 and 2), south (regions 3 and 4), Hood Canal (region 5). You can specify that some subpopulations share parameters while others do not.  You do this by using a vector of factors for the constraints: 
<<eval=F,keep.source=TRUE>>=
Q.constraint=factor(c("coastal","interior","interior"))
U.constraint=factor(c("puget sound","puget sound","hood canal"))
R.constraint=factor(c("boat","boat","plane","plane","plane"))
@
When \verb@Q.constraint@ and \verb@U.constraint@ are vectors (passed in as a factor), as above, they specify which $x$'s share parameter values.   The factors must be a vector of length $m$, where $m$ is the number of $x$'s.  The $i$-th factor corresponds to the $i$-th $x$.  In the example above, we specified that $x_1$ has its own process variance (which we named ``coastal'') and $x_2$ and $x_3$ share a process variance value (which we named ``interior'').  For the long-term trends, we specified that $x_1$ and $x_2$ share a long-term trend (``puget sound'') while $x_3$ is allowed to have a separate trend (``hood canal'').  

When \verb@R.constraint@ is vector of factors, it specifies which $y$'s have the same observation variance.  We need a $1 \times 5$ vector here because we need to specify a value for each observation time series (there are 5).   Here we imagine that observation time series 1 and 2 are boat surveys while the others are plane surveys and we want to allow the variances to differ based on methodology.  
\end{example}

\section{Discussion}
There are a number of corners that we cut in order to have case study code that runs quickly:
\begin{itemize}
\item{We ran the code starting from one initial condition.  For a real analysis, you should start from a large number of random initial conditions and use the one that gives the highest likelihood.  Since the EM algorithm is a ``hill-climbing'' algorithm, this ensures that it does not get stuck on a local maxima.  \texttt{MARSS()} will do this for you if you pass it the argument \texttt{control=list(MCInit=TRUE)}.  This will use a Monte Carlo routine to try many different initial conditions.  See the help file on \verb@MARSS()@ for more information (by typing \verb@?MARSS@ at the \R prompt).}
\item{We assume independent observation and process errors.  Depending on your system, observation errors may be driven by large-scale environmental factors (temperature, tides, prey locations) that would cause your observation errors to covary across regions. If your observation errors strongly covary between regions and you treat them as independent, this could be bad for your analysis.  The current EM code will not handle covariance in $\RR$ when there are missing data, but even it did, separating covariance across observation versus process errors will require much data (to have any power).  In practice, the first step is to think hard about what drives sightability for your species and what are the relative levels of process and observation variance.  You may be able to subsample your data in a way that will make the observation errors more independent.}
\item{The \verb@MARSS()@ argument \verb@control@ specifies the options for the EM algorithm. We left the default tolerance, \texttt{abstol=0.01}.  You would want to set this lower, e.g. \texttt{abstol=0.0001}, for a real analysis.  You will need to up the \texttt{maxit} argument correspondingly.}
\item{We used the large-sample approximation for AIC\index{model selection!AIC} instead of a bootstrap AIC\index{model selection!bootstrap AIC} that is designed to correct for small sample size in state-space models.  The bootstrap metric, AICb, takes a long time to run. Use the call \verb@MARSSaic(kem, output=c("AICbp"))@ to compute AICb\index{model selection!bootstrap AIC, AICbp}.  We could have shown AICc\index{model selection!AICc}, which is the small-sample size corrector for non-state-space models.  Type \verb@kem$AICc@ to get that.}
\end{itemize}

Finally, in a real (maximum-likelihood) analysis, one needs to be careful not to dredge the data.  The temptation is to look at the data and pick a population structure that will fit that data.  This can lead to including models in your analysis that have no biological basis.  In practice, we spend a lot of time discussing the population structure with biologists working on the species and review all the biological data that might tell us what are reasonable structures.  From that, a set of model structures to use are selected.   Other times, a particular model structure needs to be used because the population structure is not in question rather it is a matter of using that pre-specified structure and using all the data to get parameter estimates for forecasting.  

\section*{Results table}
\bigskip

\begin{center}
\begin{tabular}{c|c|c|c|c|c|c}
     & &  pop. growth   &   process  & K   & log-like     &   \\
Ex. & &  rate    &   variance  & \verb@kem$num.@  & \verb@kem$@     & AIC  \\
&  &  \verb@kem$par$U@    &   \verb@kem$par$Q@  & \verb@params@ & \verb@logLik@ & \verb@kem$AIC@  \\	
\hline
1& one population                  &   &  &    &   &\\
& different obs. vars  &   &  &    &   &\\
& uncorrelated   &   &  &    &   &\\
\hline
2& one population                  &   &  &    &   &\\
& identical obs vars  &   &  &    &   &\\
& uncorrelated   &   &  &    &   &\\
	\hline
3& N+S subpops                  &   &  &    &   &\\
& identical obs vars  &   &  &    &   &\\
& uncorrelated;   &   &  &    &   &\\
\hline
4& 5 subpops                  &   &  &    &   &\\
& unique obs vars  &   &  &    &   &\\
& $u$'s + $q$'s identical   &   &  &    &   &\\
\hline
5& N+S subpops                   &   &  &    &   &\\
& unique obs vars  &   &  &    &   &\\
& $u$'s + $q$'s identical   &   &  &    &   &\\
\hline
6& PS + HC subpops                  &   &  &    &   &\\
& unique obs vars &   &  &    &   &\\
& $u$'s + $q$'s unique   &   &  &    &   &\\
\hline
7& N + S + HC subpops                  &   &  &    &   &\\
& unique obs vars  &   &  &    &   &\\
& $u$'s + $q$'s unique   &   &  &    &   &\\
\hline
\end{tabular}
\bigskip
\end{center}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For AIC, only the relative differences matter\index{model selection!AIC}.  A difference of 10 between two AICs means substantially more support for the model with lower AIC.  A difference of 30 or 40 between two AICs is very large.  
 
\section*{Questions}
\begin{enumerate}
	\item  Do different assumptions about whether the observation error variances are all identical versus different affect your estimate of the long-term population growth rate ($u$)?  You may want to rerun examples 3-7 with the \texttt{R.constraint} changed.  \texttt{R.constraint="diagonal and unequal"} means measurement variances all different versus \texttt{"diagonal and equal"}.
	\item Do assumptions about the underlying structure of the population affect your estimates of $u$?   Structure here means number of subpopulations and which areas are in which subpopulation.  
	\item The confidence intervals for the first two analyses are very tight because the estimate process variance was very small, \verb@kem1$par$Q@.  Why do you think process variance ($q$) was forced to be so small?  [Hint: We are forcing there to be one and only one true population trajectory and all the observation time series have to fit that one time series. Look at the AICs too.]
\end{enumerate}

<<Reset, echo=F>>=
options(prompt="> ", continue="+ ")
@

<<label=Cs2_Exercises5_7, fig=FALSE, eval=TRUE, echo=FALSE, results=hide>>=
#Exercise 5
Z.constraint=factor(c(1,1,2,2,2))
U.constraint="unequal"
Q.constraint="diagonal and unequal"
R.constraint="diagonal and unequal"
kem = MARSS(dat, constraint=list(Z=Z.constraint, U=U.constraint, Q=Q.constraint, R=R.constraint) )

#Exercise 6
Z.constraint=factor(c(1,1,1,1,2))
U.constraint="unequal"
Q.constraint="equalvarcov"
R.constraint="diagonal and unequal"
kem = MARSS(dat, constraint=list(Z=Z.constraint, U=U.constraint, Q=Q.constraint, R=R.constraint) )

#Exercise 7
Z.constraint=factor(c(1,1,2,2,3))
U.constraint="unequal"
Q.constraint="diagonal and unequal"
R.constraint="diagonal and unequal"
kem = MARSS(dat, constraint=list(Z=Z.constraint, U=U.constraint, Q=Q.constraint, R=R.constraint) )
@
