\chapter{Algorithms used in the MARSS package}\label{chap:algorithms}

\section{Kalman filter and smoother}\index{estimation!Kalman filter}\index{estimation!Kalman smoother}\label{sec:kalmanfilter}

The MARSS model (Equation \ref{eqn:marss}) is a linear dynamical system in discrete time.  In 1960, Rudolf Kalman published the Kalman filter \citep{Kalman1960}, a recursive algorithm that solves for the expected value of the hidden state(s) at time $t$ conditioned on the data up to time $t$: $\E(\XX_t|\yy_1^t)$.  The Kalman filter gives the optimal (lowest mean square error) estimate of the unobserved $\xx_t$ based on the observed data up to time $t$ for this class of linear dynamical system.  The Kalman smoother \citep{Rauchetal1965} solves for the expected value of the hidden state(s) conditioned on all the data: $\E(\XX_t|\yy_1^T)$.  If the errors in the stochastic process are Gaussian, then the estimators from the Kalman filter and smoother are also the maximum-likelihood estimates.  

However, even if the the errors are not Gaussian, the estimators are optimal in the sense that they are estimators with the least variability possible.  This robustness is one reason the Kalman filter is so powerful---it provides well-behaving estimates of the hidden states for all kinds of multivariate autoregressive processes, not just Gaussian processes.  The Kalman filter and smoother are widely used in time-series analysis, and there are many textbooks covering it and its applications.  In the interest of giving the reader a single point of reference, we use \citet{ShumwayStoffer2006} as our reference and adopt their notation (for the most part).

The \verb@MARSSkf@\index{functions!MARSSkf} function provides the Kalman filter and smoother and has the following outputs:
\begin{description}
\item[\texttt{xtt1 }] The expected value of $\XX_t$ conditioned on the data up to time $t-1$.
\item[\texttt{xtt  }] The expected value of $\XX_t$ conditioned on the data up to time $t$.
\item[\texttt{xtT  }] The expected value of $\XX_t$ conditioned on all the data from time $1$ to $T$.  This the smoothed state estimate.
\item[\texttt{Vtt1 }] The variance of $\XX_t$ conditioned on the data up to time $t-1$.  Denoted $P_t^{t-1}$ in section 6.2 in \citet{ShumwayStoffer2006}.
\item[\texttt{Vtt  }] The variance of $\XX_t$ conditioned on the data up to time $t$. Denoted $P_t^t$ in section 6.2 in \citet{ShumwayStoffer2006}.
\item[\texttt{VtT  }] The variance of $\XX_t$ conditioned on all the data from time $1$ to $T$.
\item[\texttt{Vtt1T}] The covariance of $\XX_t$ and $\XX_{t-1}$ conditioned on all the data, $1$ to $T$.
\item[\texttt{Kt   }] The Kalman gain.  This is part of the update equations and relates to the amount \verb@xtt1@ is updated by the data at time $t$ to produce \verb@xtt@.
\item[\texttt{J    }] This is similar to the Kalman gain but is part of the Kalman smoother.  See Equation 6.49 in \citet{ShumwayStoffer2006}.
\item[\texttt{Innov}] This has the innovations at time $t$, defined as $\ep_t \equiv \yy_t$-$\E(\YY_t)$.  These are the residuals, the difference between the data and their predicted values.  See Equation 6.24 in \citet{ShumwayStoffer2006}.
\item[\texttt{Sigma}] This has the $\Sigma_t$, the variance-covariance matrices for the innovations at time $t$.  This is used for the calculation of confidence intervals, the s.e. on the state estimates and the likelihood.  See Equation 6.25 in \citet{ShumwayStoffer2006} for the $\Sigma_t$ calculation.
\item[\texttt{logLik}] The log-likelihood of the data conditioned on the model parameters.  See the section below on the likelihood calculation.
\end{description}

\section{The exact likelihood}\index{likelihood}
\label{sec:exactlikelihood}
The likelihood of the data given a set of MARSS parameters is part of the output of the \verb@MARSSkf@\index{functions!MARSSkf}
 function.  The likelihood computation is based on the innovations form of the likelihood\index{likelihood!innovations algorithm} \citep{Schweppe1965} and uses the output from the Kalman filter:
\begin{equation}
\log \Lik(\Theta | data) = -\frac{N}{2\log(2\pi)} - \frac{1}{2}\left( \sum_{t=1}^T\log |\Sigma_t| + \sum_{t=1}^T (\ep_t)^\top \Sigma_t^{-1} \ep_t \right)
\label{eq:loglike}
\end{equation}
where $N$ is the total number of data points, $\ep_t$ is the innovations at time $t$ and $|\Sigma_t|$ is the determinant of the innovations variance-covariance matrix at time $t$.  Reference Equation 6.62 in \citet{ShumwayStoffer2006}. However there are a few differences between the log-likelihood output by \verb@MARSSkf@\index{functions!MARSSkf} and that described in \citet{ShumwayStoffer2006}.

The standard likelihood calculation (Equation 6.62 in \citet{ShumwayStoffer2006}) is biased when there are missing values in the data, and the missing data modifications\index{likelihood!missing value modifications} discussed in Section 6.4 in \citet{ShumwayStoffer2006} do not correct for this bias. \citet{Harvey1989}, Section 3.4.7, discusses at length that the standard missing values correction leads to an inexact likelihood when there are missing values. The bias is minor if there are few missing values, but it becomes severe as the number of missing values increases.  Many ecological datasets may have over 25\% missing values and this level of missing values leads to a very biased likelihood if one uses the inexact formula.  \citet{Harvey1989} provides some non-trivial ways to compute the exact likelihood.  

We use instead the exact likelihood correction for missing values that is presented in Section 12.3 in \citet{BrockwellDavis1991}\index{missing values!likelihood correction}\index{likelihood!and missing values}.  This solution is straight-forward to implement.  The correction involves the following changes to $\ep_t$ and $\Sigma_t$ in the Equation \ref{eq:loglike}.  Suppose the value $y_{i,t}$ is missing.  First, the corresponding $i$-th value of $\ep_t$ is set to 0.  Second, the $i$-th diagonal value of $\Sigma_t$ is set to 1 and the off-diagonal elements on the $i$-th column and $i$-th row are set to 0.

\section{Maximum-likelihood parameter estimation}

\subsection{Kalman-EM algorithm}\index{estimation!Kalman-EM}

The MARSS package provides a maximum-likelihood algorithm which uses an Expectation-Maximization (EM) algorithm (function \verb@MARSSkem@\index{functions!MARSSkem}) with the Kalman smoother.  EM algorithms are widely used algorithms that extend maximum-likelihood estimation to cases where there are hidden random variables in a model 
\citep{Dempsteretal1977, Harvey1989, HarveyShephard1993, McLachlanKrishnan2008}.  

The EM algorithm finds the maximum-likelihood estimates of the parameters in a MARSS model using an iterative process.  Starting with an initial set of parameters\footnote{You can choose these however you wish, however choosing something not too far off from the correct values will make the algorithm go faster.}, which we will denote $\hat{\Theta}_1$, an updated parameter set $\hat{\Theta}_2$ is obtaining by finding the $\hat{\Theta}_2$ that maximizes the expected value of the likelihood over the distribution of the states ($\XX$) conditioned on $\hat{\Theta}_1$:
\begin{align}\label{eqn:EM}
	\hat{\Theta}_2 &= \arg\underset{\Theta}{\max} \quad \E_{\XX|\hat{\Theta}_1}[\log L(\Theta |\YY_1^T=\yy_1^T,\XX)]\\
\nonumber
\end{align}
Then using $\hat{\Theta}_2$ in place of $\hat{\Theta}_1$ in Equation \eqref{eqn:EM}, an updated parameter set $\hat{\Theta}_3$ is calculated.  This is repeated until the expected log-likelihood stops increasing (or increases less than some set tolerance level).

Implementing this algorithm is straight-forward, hence its popularity.
\begin{enumerate}
	\item Set an initial set of parameters, $\hat{\Theta}_1$
	\item E step: using the model for the hidden states ($\XX$) and $\hat{\Theta}_1$, calculate the expected values of $\XX$ conditioned on all the data $\yy_1^T$; this is \verb@xtT@ output by \verb@MARSSkf@.  Also calculate expected values of any functions of $\XX$, $g(\XX)$, that appear in your expected log-likelihood function.
	\item M step: put those $\E(\XX | \YY_1^T=\yy_1^T, \hat{\Theta}_1)$ and $\E(g(\XX) | \YY_1^T=\yy_1^T, \hat{\Theta}_1)$ into your expected log-likelihood function in place of $\XX$ (and $g(\XX)$) and maximize with respect to $\Theta$.  This gives you $\hat{\Theta}_2$.
	\item Repeat the E and M steps until the log likelihood stops increasing.
\end{enumerate}

The EM equations in our algorithm, which we term the Kalman-EM algorithm, are extensions of those in \citet{ShumwayStoffer1982} and \citet{GhahramaniHinton1996}.  Our Kalman-EM algorithm is an extended version because our algorithm is for cases where there are constraints within the parameter matrices (shared values, diagonal structure, block-diagonal structure, ...) and where there are fixed values within the parameter matrices.  \citet{Holmes2010} gives the full derivation of our EM algorithm.  

The EM algorithm is a hill-climbing algorithm and like all hill-climbing algorithms can get stuck on local maxima\index{likelihood!troubleshooting}\index{likelihood!multimodal}.  The MARSS package includes a Monte-Carlo initial conditions searcher (function \verb@MARSSmcinit@\index{functions!MARSSmcinit}) based on \citet{Biernackietal2003} to minimize this problem.  EM algorithms are also known to get close to the maximum very quickly but then creep toward the absolute maximum.  Quasi-Newton methods\index{estimation!Newton methods} find the absolute maximum much faster, but they can be  sensitive to initial conditions.  We have found that with MARSS models, quasi-Newton methods (at least using \verb@optim@\index{functions!optim}) will sometimes converge far from the maximum even when started close to the known maximum. For this reason, the Monte Carlo initial condition search that works for EM algorithms may not work for Newton algorithms.

\section{Parametric and innovations bootstrapping}
Bootstrapping\index{bootstrap!parametric}\index{bootstrap!innovations} can be used to construct frequentist confidence intervals on the parameter estimates \citep{StofferWall1991} and to compute the small-sample AIC corrector for MARSS models \citep{CavanaughShumway1997}; the functions \verb@MARSSparamCIs@\index{functions!MARSSparamCIs} and \verb@MARSSaic@\index{functions!MARSSaic} do these computations. 

The \verb@MARSSboot@\index{functions!MARSSboot} function provides both parametric and innovations bootstrapping of MARSS models.  
The innovations\index{bootstrap!innovations} bootstrap algorithm by \citet{StofferWall1991}  bootstraps the model residuals (the innovations).  This is a semi-parametric bootstrap since is uses, partially, the maximum-likelihood parameter estimates.  This algorithm cannot be used if there are missing values in the data.  Also for short time series, it gives biased bootstraps because one cannot resample the first few innovations.  

\verb@MARSSboot@ also provides a fully parametric\index{bootstrap!parametric} bootstrap.  This uses the maximum-likelihood MARSS parameters to simulate data from which bootstrap parameter estimates are obtained.  Our research \citep{HolmesWard2010} indicates that this provides unbiased bootstrap parameter estimates, and it works with datasets with missing values\index{missing values!and parametric bootstrap}.  Lastly, \verb@MARSSboot@ also output parameters sampled from a numerically estimated Hessian matrix.  

\section{Simulation and forecasting}
The \verb@MARSSsimulate@\index{functions!MARSSsimulate} function simulates\index{simulation} from a MARSS model using a list of parameter matrices.  It use the \verb@mvrnorm@ function to produce draws of the process and observation errors from multivariate normal distributions for each time step.  

\section{Model selection}\index{model selection}
The package provides a \verb@MARSSaic@\index{functions!MARSSaic} function for computing AIC\index{model selection!AIC}, AICc and AICb.  The latter is a small-sample corrector for autoregressive state-space models.  The bias problem with AIC and AICc for short time-series data has been shown in \citet{CavanaughShumway1997} and \citet{HolmesWard2010}.  AIC and AICc tend to select overly complex MARSS models when the time-series data are short.  AICb corrects this bias.  The algorithm for a non-parametric AICb\index{model selection!AICb} is given in \citet{CavanaughShumway1997}.  Their algorithm uses the innovations\index{bootstrap!innovations} bootstrap \citep{StofferWall1991}, which means it cannot be used when there are missing data.  We added a parametric\index{bootstrap!parametric} AICb \citep{HolmesWard2010}, which uses a parametric bootstrap. This algorithm allows one to compute AICb when there are missing data\index{missing values!and AICb} and it provides unbiased AIC even for short time series.  See \citet{HolmesWard2010} for discussion and testing of parametric AICb for MARSS models. 

AICb\index{model selection!AICb} is comprised of the familiar AIC fit term, $-2 \log L$, plus a penalty term that is the mean difference between the log likelihood the data under the bootstrapped maximum-likelihood parameter estimates and the log likelihood of the data under the original maximum-likelihood parameter estimate:
\begin{equation}
	AICb = -2 \log \Lik(\hat{\Theta} | \yy) + 2 \bigg( \frac{1}{N_b}\sum_{i=1}^{N_b} -\log \frac{\Lik(\hat{\Theta}^*(i) | \yy)}{\Lik(\hat{\Theta} | \yy)}\bigg)
\label{eq:AICb}
\end{equation}
where $\hat{\Theta}$ is the maximum-likelihood parameter set under the original data $\yy$, $\hat{\Theta}^*(i)$ is a maximum-likelihood parameter set estimated from the $i$-th bootstrapped data set $\yy^*(i)$, and $N_b$ is the number of bootstrap data sets.  It is important to notice that the likelihood in the AICb\index{model selection!AICb} equation is $\Lik(\hat{\Theta}^* | \yy)$ not $\Lik(\hat{\Theta}^* | \yy^*)$.  In other words, we are taking the average of the likelihood of the original data given the bootstrapped parameter sets. 
