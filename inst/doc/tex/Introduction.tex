\chapter{The MARSS package}

MARSS stands for Multivariate Auto-Regressive(1) State-Space. The MARSS package is designed for linear MARSS models with Gaussian errors\index{MARSS model}.  This class of model is extremely important in the study of linear stochastic dynamical systems, and these models are important in many different fields, including economics, engineering, genetics, physics and ecology.  Appendix \ref{chap:SSreferences} gives a selection of textbooks on MARSS models.   

A MARSS model, with Gaussian errors, takes the form:
\begin{subequations}\label{eqn:marss}
\begin{gather}
\xx_t = \BB\xx_{t-1} + \uu + \vv_t, \text{ where } \vv_t \sim \MVN(0,\QQ) \label{eqn:marssx}\\
\yy_t = \ZZ\xx_t + \aa + \ww_t, \text{ where } \ww_t \sim \MVN(0,\RR) \label{eqn:marssy}\\
\xx_1 \sim \MVN(\pipi,\VV_1) \label{eqn:marssx1}
\end{gather}
\end{subequations}
The model includes random variables, parameters and data:
\begin{itemize}
	\item[$\xx_t$] is a $m \times 1$ column vector of the hidden states at time $t$.  It is a realization of the random variable $\XX_t$.
	\item[$\vv_t$] is a $m \times 1$ column vector of the process errors at time $t$.  It is a realization from a multivariate normal random variable with mean 0 and $\Sigma=\QQ$.
	\item[$\yy_t$] is a $n \times 1$ column vector of the observed data at time $t$.
	\item[$\ww_t$] is a $n \times 1$ column vector of the non-process errors at time $t$.  It is a realization from a multivariate normal random variable with mean 0 and $\Sigma=\RR$.
	\item[$\BB$] is a parameter and is a $m \times m$ matrix.
	\item[$\uu$] is a parameter and is a $m \times 1$ column vector.
	\item[$\QQ$] is a parameter and is a $m \times m$ variance-covariance matrix.
	\item[$\ZZ$] is a parameter and is a $n \times m$ matrix.
	\item[$\aa$] is a parameter and is a $n \times 1$ column vector.
	\item[$\RR$] is a parameter and is a $n \times n$ variance-covariance matrix.
	\item[$\pipi$] is either a parameter or a fixed prior. It is a $m \times 1$ matrix.
	\item[$\VV_1$] is a fixed value. It is a $m \times m$ variance-covariance matrix.
\end{itemize}

The meaning of the parameters in the MARSS models depends on the application for which the MARSS model is being used.  In the case studies, we show examples of MARSS models used to analyze population count data and animal tracking data, and Appendix \ref{chap:SSreferences} gives a selection of papers from the ecological literature.  However, the MARSS package is not specific to population modeling applications.  The functions in the MARSS package are generic functions for fitting MARSS models have forms like: 

\section{What does the MARSS package do?}
The MARSS package is designed to fit unconstrained and constrained MARSS models.  A constrained MARSS model is one in which some of the parameters are constrained in the sense that they have fixed, free and/or shared values.  For example, let $\MM$ and $\mm$ be arbitrary matrix and column vector parameters.  The MARSS package allows one to specify and fit models where $\MM$ and $\mm$ have shared values and fixed values.  For example,
\begin{equation*}
\MM=
\begin{bmatrix}
a&0.9&c\\
-1.2&a&0\\
0&c&b
\end{bmatrix}
\text{ and }
\mm=
\begin{bmatrix}
d\\
d\\
e\\
2.2
\end{bmatrix}
\end{equation*}

Version 1.x of the MARSS package fits models via maximum-likelihood using a Kalman-EM algorithm\footnote{The package can also fit models via quasi-Newton methods based on \R's \texttt{optim} function. This can be especially useful for finishing off a Kalman-EM estimate  when the data to parameter ratio is high.  However, when the ratio of data to parameters is low (as in many ecological applications), the quasi-Newton algorithm tends to be fragile and sensitive to initial conditions.}.  The Kalman-EM algorithm is used because it gives robust estimation for datasets replete with missing values and for models with various constraints.  The MARSS package also supplies functions for bootstrap and approximate confidence intervals, parametric and non-parametric bootstrapping, model selection (AIC and bootstrap AIC), simulation, and bootstrap bias correction.   Version 1.0 does not allow $\BB$ or $\ZZ$ to be estimated.  Version 2.0 is currently being tested and it will allow $\BB$ and $\ZZ$ estimation along with less constrained forms of $\QQ$ and $\RR$.

\section{How to get started (quickly)}

Install the MARSS package and then type \texttt{library(MARSS)} at the command line to load the package.  Read the first 2-4 pages of Chapter \ref{chap:MARSS} then read through a couple of the examples in Chapter \ref{Examples}.  Get your data into a matrix (not dataframe) with time going across the columns and any non-$yy$ columns (like year) removed.  Replace any missing time steps with a missing value holder (like NA or -99).  Write your model down on paper and identify which parameters correspond to $\BB$, $\uu$, $\QQ$, $\ZZ$, $\aa$, and $\RR$ in the MARSS model (Equation \ref{eqn:marss}).  Call the \texttt{MARSS()} function (Chapter \ref{chap:MARSS}) using your data and using the \texttt{constraint} argument to specify the form of the parameters (Chapter \ref{chap:MARSS} shows the options for the forms).

\section{Important notes about the algorithms}

MARSS 1.0 provides maximum-likelihood via an EM algorithm using the Kalman filter/smoother.  All code is in native \R.  Thus the model fitting is slow (relatively).  Writing the algorithms in C would speed them up considerably, but we have no plans to do that.  EM algorithms will quickly get in the vicinity of the maximum likelihood, but the final approach to the maximum is generally slow relative to quasi-Newton methods.  On the flip side, EM algorithms are quite robust to initial conditions choices unlike quasi-Newton methods which can be sensitive to initial condition choices.  The MARSS package allows one to use the BFGS method in \verb@optim()@\index{functions!optim} to fit MARSS models. The \texttt{DLM} package\index{DLM package} (search for it on CRAN) also provides fitting via quasi-Newton methods (and Bayesian methods).

Restricted maximum-likelihood algorithms\index{estimation!REML} are also available for AR-1 state-space models, both univariate \citep{Staplesetal2004} and multivariate \citep{Hinrichsen2009}.  REML can give parameter estimates with lower variance than plain maximum-likelihood algorithms.  However, the algorithms for REML when there are missing values are not currently available.  Another maximum-likelihood method is data-cloning which adapts MCMC algorithms used in Bayesian analysis for maximum-likelihood estimation \citep{Leleetal2007}.  

Data with cycles, from say internal dynamical interactions, are difficult to analyze, and both REML and Kalman-EM approaches will give poor estimates for this type of data.  The slope method \citep{Holmes2001}, is more ad-hoc but is relatively robust to those problems.  \citet{Holmesetal2007} used the slope method in a large study of data from endangered and threatened species; \citet{EllnerHolmes2008}  showed that the slope estimates are close to the theoretical minimum uncertainty.  However estimates using the slope method are not easily extended to multi-variate data and it is not a true maximum-likelihood method. 

Missing values\index{missing values} are seamlessly accommodated with the MARSS package.  Simply specify the way missing values are denoted in the data set (default is \verb@miss.value=-99@).  The likelihood computations are exact and will deal appropriately with missing values.  The presence of missing values, however, limits the $\RR$ matrix to being a diagonal matrix (if estimated) and the $\ZZ$ matrix to being fixed.  In addition, no innovations\footnote{referring to the non-parametric bootstrap used in the package} bootstrapping can be done if there are missing values.  Instead parametric bootstrapping must be used.

You should be aware that maximum-likelihood estimates of variance in MARSS models are fundamentally biased, regardless of the algorithm used.  This bias is more severe when one or the other of $\RR$ or $\QQ$ is very small, and the bias does not go to zero as sample size goes to infinity.  The bias arises because variance is constrained to be positive.  Thus if $\RR$ or $\QQ$ is essentially zero, the mean estimate will not be zero and thus the estimate  will be biased high while the corresponding bias of the other variance will be biased low.  You can generate unbiased variance estimates using a bootstrap estimate of the bias.  The function \texttt{MARSSparamCIs()}\index{functions!MARSSparamCIs} will do this.  However be aware that adding an {\it estimated} bias to a parameter estimate will lead to an increase in the variance of your parameter estimate.  The amount of variance added will depend on sample size.

\section{Troubleshooting}
There are two numerical errors and warnings that you may see when fitting MARSS models: ill-conditioning and degeneracy\index{errors!ill-conditioned}\index{errors!degenerate}\index{likelihood!troubleshooting}. The Kalman and EM algorithms need inverses of matrices. If those matrices become ill-conditioned, for example all elements are close to the same value, then the algorithm becomes unstable.  MARSS will print warning messages if the algorithm is becoming unstable and you can set \verb@control$trace=1@, to see details of where the algorithm is becoming unstable.  Whenever possible, you should avoid using shared $\pipi$ values in your model\footnote{An example of a $\pipi$ with shared values is $\pipi=\bigl[\begin{smallmatrix} a\\a\\a \end{smallmatrix} \bigr]$.}.  The way our algorithm deals with $\VV_1$ tends to make this case unstable, especially if $\RR$ is not diagonal.  In general, estimation of a non-diagonal $\RR$ is more difficult, more prone to ill-conditioning, and more data-hungry.

The second numerical error you may see is a degeneracy warning.  This means that one of the elements on the diagonal of your $\QQ$ or $\RR$ matrix are going to zero (are degenerate).  It will take the EM algorithm forever to get to zero.  Since the likelihood can spike up very fast near a degenerate solution, the log-likelihood value reported will be too small because it will include estimates of the degenerate $\QQ$ or $\RR$ diagonal elements that are very small but nonetheless non-zero.  BFGS will have the same problem, although it will often get a bit closer to the degenerate solution.  If you are using \verb@method="kem"@, MARSS will warn you if it looks like the solution is degenerate and you can use the function \verb@find.degenerate()@\index{functions!find.degenerate} to find the degenerate elements or look at the \verb@$errors@ element of the output.  

The algorithms in the MARSS 1.0 package are designed for cases where the $\QQ$ and $\RR$ diagonals are all non-minuscule.  For example, the EM update equation for $\UU$ will grind to a halt (not update $\UU$) if $\QQ$ is tiny (like 1E-7).  Conversely, the BFGS equations are likely to miss the maximum-likelihood when $\RR$ is tiny because then the likelihood surface becomes hyper-sensitive to $\pipi$.   The solution is to use the degenerate likelihood function for the likelihood calculation and the EM update equations.  However this solution will not be implemented until MARSS 2.0.  These concerns affect the likelihood value reported at the maximum-likelihood parameters. The actual parameter estimates will change very little on the absolute scale, so your point estimates, confidence intervals, bias estimates, etc. are still valid.  Model selection though will be dubious because the likelihoods reported by MARSS will not be the real maximums\index{likelihood!parameters not converged}.